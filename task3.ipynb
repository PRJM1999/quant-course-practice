{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Credit Risk Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task:\n",
    "\n",
    "The risk manager has collected data on the loan borrowers. The data is in tabular format, with each row providing details of the borrower, including their income, total loans outstanding, and a few other metrics. There is also a column indicating if the borrower has previously defaulted on a loan. You must use this data to build a model that, given details for any loan described above, will predict the probability that the borrower will default (also known as PD: the probability of default). Use the provided data to train a function that will estimate the probability of default for a borrower. Assuming a recovery rate of 10%, this can be used to give the expected loss on a loan.\n",
    "\n",
    "- You should produce a function that can take in the properties of a loan and output the expected loss.\n",
    "- You can explore any technique ranging from a simple regression or a decision tree to something more advanced. You can also use multiple methods and provide a comparative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution:\n",
    "\n",
    "Using Object-Oriented Programming (OOP) principles in this solution provides modular, maintainable, and scalable code that streamlines the loan risk assessment process.\n",
    "\n",
    "### Data Processor\n",
    "\n",
    "-   Purpose:\n",
    "    -   The `DataProcessor` class serves to prepare the data for model training.\n",
    "-   Functionalities:\n",
    "    -   Preprocessing: Drops unnecessary columns (e.g., `customer_id`).\n",
    "    -   Splitting: Divides data into training and testing sets.\n",
    "    -   Normalization: Scales the data features using `StandardScaler` for better model performance.\n",
    "-   Key Components:\n",
    "    -   `preprocess()`: Removes irrelevant columns from the data.\n",
    "    -   `split_data()`: Segregates the data into training and test subsets.\n",
    "    -   `normalize()`: Uses `StandardScaler` to standardize feature values.\n",
    "    -   `process()`: A high-level function calling the other three functions to streamline data processing.\n",
    "\n",
    "### Model Development (LogisticRegressionModel class)\n",
    "\n",
    "-   Purpose:\n",
    "    -   Defines and trains a logistic regression model to predict default probabilities.\n",
    "-   Functionalities:\n",
    "    -   Training: Trains a logistic regression model using training data.\n",
    "    -   Prediction: Forecasts class labels and probabilities for given input data.\n",
    "-   Key Components:\n",
    "    -   `train()`: Fitting the logistic regression model using training data.\n",
    "    -   `predict()`: Provides the predicted class labels for input data.\n",
    "    -   `predict_proba()`: Computes the probabilities of default for input data.\n",
    "\n",
    "### Risk Calculator (LoanRiskCalculator)\n",
    "\n",
    "-   Purpose:\n",
    "    -   Calculates the expected financial loss based on the probability of default and given loan attributes.\n",
    "-   Functionalities:\n",
    "    -   Expected Loss Calculation: Uses the formula: PD * EAD * (1 - RR), where:\n",
    "        -   PD: Probability of Default\n",
    "        -   EAD: Exposure At Default\n",
    "        -   RR: Recovery Rate\n",
    "-   Key Components:\n",
    "    -   `calculate_expected_loss()`: Determines the expected loss based on the model's predictions and loan data.\n",
    "\n",
    "### Sample Input (SampleInput class)\n",
    "\n",
    "-   Purpose:\n",
    "    -   Provides an easy way to prepare user input for model predictions, especially normalization.\n",
    "-   Functionalities:\n",
    "    -   Input Transformation: Converts raw loan properties into a suitable format for prediction.\n",
    "    -   Normalization: Scales the input using the same scaler from `DataProcessor`.\n",
    "-   Key Components:\n",
    "    -   `get_normalized_input()`: Transforms and scales raw input values for model prediction.\n",
    "\n",
    "### Expected Loss Calculation\n",
    "\n",
    "-   Steps:\n",
    "    1.  Manually input loan properties.\n",
    "    2.  Use the `SampleInput` class to prepare and normalize these values.\n",
    "    3.  Utilize the `LoanRiskCalculator` class to compute the expected loss.\n",
    "    4.  Output: Presents the expected financial loss for the given loan details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, data_path, test_size=0.2, random_state=None):\n",
    "        \"\"\"Initializes the DataProcessor object.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the CSV file containing the data.\n",
    "            test_size (float, optional): Proportion of data to be used for testing. Defaults to 0.2.\n",
    "            random_state (int, optional): Random seed for reproducibility. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.drop_columns = \"customer_id\"  # Column to drop from data\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()  # Standard scaler for data normalization\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocesses the data by dropping unnecessary columns.\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        self.data = self.data.drop(columns=self.drop_columns)\n",
    "        print(f\"Data shape after preprocessing: {self.data.shape}\")\n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"Splits the data into training and testing sets.\"\"\"\n",
    "        print(\"Splitting data...\")\n",
    "        X = self.data.drop(columns=\"default\")  # Feature columns\n",
    "        y = self.data[\"default\"]               # Target column\n",
    "        # Splitting data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state)\n",
    "        print(f\"Training data shape: {self.X_train.shape}, Testing data shape: {self.X_test.shape}\")\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Normalizes the training and testing data using StandardScaler.\"\"\"\n",
    "        print(\"Normalizing data...\")\n",
    "        # Fit the scaler using training data and transform\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "        # Transform the testing data\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Processes the data by performing preprocessing, splitting, and normalization.\"\"\"\n",
    "        self.preprocess()\n",
    "        self.split_data()\n",
    "        self.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, X_train, y_train):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LogisticRegressionModel(BaseModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def train(self, X_train, y_train) -> LogisticRegression:\n",
    "        \"\"\"Train the Logistic Regression model.\"\"\"\n",
    "        print(\"Training the Logistic Regression model...\")\n",
    "        self.model.fit(X_train, y_train)\n",
    "        return self.model\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class labels for given data.\"\"\"\n",
    "        print(\"Predicting class labels...\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict the probabilities for given data.\"\"\"\n",
    "        print(\"Predicting probabilities...\")\n",
    "        return self.model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoanRiskCalculator:\n",
    "\n",
    "    def __init__(self, model: BaseModel, recovery_rate=0.1):\n",
    "        \"\"\"\n",
    "        :param model: An instance of a model implementing the BaseModel protocol.\n",
    "        :param recovery_rate: The recovery rate for the loan. Defaults to 0.1 (10%).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.RR = recovery_rate\n",
    "\n",
    "    def calculate_expected_loss(self, X, EAD):\n",
    "        \"\"\"\n",
    "        Calculate the expected loss for a given set of loan properties.\n",
    "\n",
    "        :param X: The input data for which to predict the probability of default.\n",
    "        :param EAD: Exposure At Default, the amount at risk in case of a default.\n",
    "        :return: Expected loss for the given data.\n",
    "        \"\"\"\n",
    "        print(\"Calculating expected loss...\")\n",
    "        PD_array = self.model.predict_proba(X)\n",
    "        PD = PD_array[0][1]  # Accessing the probability for the positive class\n",
    "        expected_loss = PD * EAD * (1 - self.RR)\n",
    "        print(f\"Probability of Default (PD) for the input: {PD:.5f}\")\n",
    "        return expected_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleInput:\n",
    "    def __init__(self, processor: DataProcessor):\n",
    "        \"\"\"Initializes the SampleInput with a reference to the data processor.\n",
    "        \n",
    "        Args:\n",
    "            processor (DataProcessor): Reference to the data processor for normalization.\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.columns = [\n",
    "            \"credit_lines_outstanding\",\n",
    "            \"loan_amt_outstanding\",\n",
    "            \"total_debt_outstanding\",\n",
    "            \"income\",\n",
    "            \"years_employed\",\n",
    "            \"fico_score\"\n",
    "        ]\n",
    "    \n",
    "    def get_normalized_input(self, values: list) -> np.ndarray:\n",
    "        \"\"\"Convert raw input values to a DataFrame and normalize them.\n",
    "        \n",
    "        Args:\n",
    "            values (list): Raw input values.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Normalized values.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame([values], columns=self.columns)\n",
    "        return self.processor.scaler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Loss Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Data shape after preprocessing: (10000, 7)\n",
      "Splitting data...\n",
      "Training data shape: (8000, 6), Testing data shape: (2000, 6)\n",
      "Normalizing data...\n",
      "Training the Logistic Regression model...\n",
      "Calculating expected loss...\n",
      "Probability of Default (PD) for the input: 0.00153\n",
      "Expected Loss: $6.899230555657993\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and process the data\n",
    "processor = DataProcessor('Loan_Data.csv')\n",
    "processor.process()\n",
    "\n",
    "# 2. Train the logistic regression model with processed data\n",
    "lr_model = LogisticRegressionModel().train(processor.X_train, processor.y_train)\n",
    "\n",
    "# 3. Initialize the LoanRiskCalculator\n",
    "calculator = LoanRiskCalculator(model=lr_model)\n",
    "\n",
    "# 4. Create a sample input and calculate expected loss\n",
    "# Initialize the SampleInput class\n",
    "sample_input = SampleInput(processor)\n",
    "\n",
    "# Manually input values for the prediction\n",
    "sample_values = [\n",
    "    1,              # credit_lines_outstanding\n",
    "    5000,           # loan_amt_outstanding\n",
    "    10000,          # total_debt_outstanding\n",
    "    40000,          # income\n",
    "    3,              # years_employed\n",
    "    580             # fico_score (credit score)\n",
    "]\n",
    "\n",
    "# Obtain normalized values using SampleInput class\n",
    "X_sample_normalized = sample_input.get_normalized_input(sample_values)\n",
    "\n",
    "# 5. Calculate expected loss using the manually input values\n",
    "EAD_sample = 5000  # Just an example value for Exposure At Default\n",
    "expected_loss = calculator.calculate_expected_loss(X_sample_normalized, EAD_sample)\n",
    "\n",
    "print(f\"Expected Loss: ${expected_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
