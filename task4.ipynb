{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Bucket FICO Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Charlie aims to build a model that is applicable to future datasets. The challenge is to produce a general methodology for bucketing FICO scores, such that these buckets serve as input labels to the model. The objective is to delineate boundaries that best capture and summarize the FICO score distribution. This process is termed \"quantization.\"\n",
    "\n",
    "Given a FICO score, the target is to map it to a rating in which a lower rating implies a better credit score.\n",
    "\n",
    " ## Intial Approach\n",
    "\n",
    "The first step taken was a simple and straightforward bucketing based on equal intervals:\n",
    "\n",
    "This function took the start and end values of the FICO scores, the number of desired buckets (k), and the dataframe (df). It then computed equal-sized buckets within this range. Each bucket's log-likelihood was calculated under the assumption of a normal distribution, which was subsequently used to evaluate the bucket's fit to the data.\n",
    "\n",
    "## Optimised Solution\n",
    "\n",
    "To improve the simple bucketing approach and get a better fit for the data, dynamic adjustments were made to the bucket boundaries. The objective was to find boundaries that would minimize the log-likelihood of the FICO scores falling within them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equally Split\n",
    "\n",
    "Investigate the Log Likelihood if the buckets were split equally to give some background context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets for 0-600 (Format: (Start, End, Log Likelihood)):\n",
      "(0, 119, nan)\n",
      "(120, 239, nan)\n",
      "(240, 359, nan)\n",
      "(360, 479, -227.43410802662515)\n",
      "(480, 600, -12527.639323549474)\n",
      "\n",
      "Buckets for 600-850 (Format: (Start, End, Log Likelihood)):\n",
      "(600, 649, -12652.791737625666)\n",
      "(650, 699, -11147.419966664507)\n",
      "(700, 749, -5026.754543115131)\n",
      "(750, 799, -1009.2452004585322)\n",
      "(800, 850, -133.0821644404997)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickmontgomery/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/patrickmontgomery/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/patrickmontgomery/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/patrickmontgomery/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/Users/patrickmontgomery/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_buckets(start, end, k, df):\n",
    "    bucket_size = (end - start) // k\n",
    "    buckets = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        lower_bound = start + i * bucket_size\n",
    "        if i == k - 1:  # For the last bucket, ensure we capture the end value\n",
    "            upper_bound = end\n",
    "        else:\n",
    "            upper_bound = lower_bound + bucket_size - 1\n",
    "        \n",
    "        # Filter the dataframe for FICO scores within the current bucket\n",
    "        bucket_data = df[(df['fico_score'] >= lower_bound) & (df['fico_score'] <= upper_bound)]['fico_score'].values\n",
    "        \n",
    "        # Calculate log likelihood assuming a normal distribution\n",
    "        mu = np.mean(bucket_data)\n",
    "        sigma = np.std(bucket_data)\n",
    "        n = len(bucket_data)\n",
    "        log_likelihood = -0.5 * n * np.log(2 * np.pi * sigma**2) - (1 / (2 * sigma**2)) * np.sum((bucket_data - mu)**2)\n",
    "        \n",
    "        buckets.append((lower_bound, upper_bound, log_likelihood))\n",
    "        \n",
    "    return buckets\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('Loan_Data.csv')\n",
    "\n",
    "buckets_0_600 = create_buckets(0, 600, 5, df)\n",
    "buckets_600_850 = create_buckets(600, 850, 5, df)\n",
    "\n",
    "print(\"Buckets for 0-600 (Format: (Start, End, Log Likelihood)):\")\n",
    "for bucket in buckets_0_600:\n",
    "    print(bucket)\n",
    "\n",
    "print(\"\\nBuckets for 600-850 (Format: (Start, End, Log Likelihood)):\")\n",
    "for bucket in buckets_600_850:\n",
    "    print(bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets for 0-600 (Format: (Start, End, Log Likelihood)):\n",
      "(0, 119, -inf)\n",
      "(120, 239, -inf)\n",
      "(240, 359, -inf)\n",
      "(360, 794, -54828.762269007704)\n",
      "(479, 600, -12553.944341609436)\n",
      "\n",
      "Buckets for 600-850 (Format: (Start, End, Log Likelihood)):\n",
      "(600, 794, -37736.59835138829)\n",
      "(479, 794, -54338.58344768431)\n",
      "(479, 794, -54338.58344768431)\n",
      "(479, 806, -54541.565314484746)\n",
      "(796, 850, -178.35289866784876)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def log_likelihood(bucket_data):\n",
    "    if len(bucket_data) == 0:\n",
    "        return float('-inf')\n",
    "    mu = np.mean(bucket_data)\n",
    "    sigma = np.std(bucket_data)\n",
    "    n = len(bucket_data)\n",
    "    return -0.5 * n * np.log(2 * np.pi * sigma**2) - (1 / (2 * sigma**2)) * np.sum((bucket_data - mu)**2)\n",
    "\n",
    "def optimize_buckets(start, end, k, df, iterations=1000):\n",
    "    bucket_size = (end - start) // k\n",
    "    buckets = [(start + i * bucket_size, start + (i + 1) * bucket_size - 1) for i in range(k)]\n",
    "    buckets[-1] = (buckets[-1][0], end)  # Ensure last bucket captures the end value\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        for idx, (lower, upper) in enumerate(buckets):\n",
    "            original_data = df[(df['fico_score'] >= lower) & (df['fico_score'] <= upper)]['fico_score'].values\n",
    "            original_ll = log_likelihood(original_data)\n",
    "\n",
    "            # Check if we can adjust the lower boundary\n",
    "            if idx > 0:\n",
    "                lower_adjusted_data = df[(df['fico_score'] >= lower-1) & (df['fico_score'] <= upper)]['fico_score'].values\n",
    "                lower_ll = log_likelihood(lower_adjusted_data)\n",
    "\n",
    "            # Check if we can adjust the upper boundary\n",
    "            if idx < len(buckets) - 1:\n",
    "                upper_adjusted_data = df[(df['fico_score'] >= lower) & (df['fico_score'] <= upper+1)]['fico_score'].values\n",
    "                upper_ll = log_likelihood(upper_adjusted_data)\n",
    "\n",
    "            # Determine which boundary adjustment (if any) reduces the log likelihood the most\n",
    "            if idx > 0 and lower_ll < original_ll:\n",
    "                buckets[idx] = (lower-1, upper)\n",
    "                original_ll = lower_ll  # update the reference likelihood\n",
    "\n",
    "            if idx < len(buckets) - 1 and upper_ll < original_ll:\n",
    "                buckets[idx] = (lower, upper+1)\n",
    "\n",
    "    # Get the log likelihoods for the adjusted buckets\n",
    "    results = []\n",
    "    for lower, upper in buckets:\n",
    "        bucket_data = df[(df['fico_score'] >= lower) & (df['fico_score'] <= upper)]['fico_score'].values\n",
    "        results.append((lower, upper, log_likelihood(bucket_data)))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('Loan_Data.csv')\n",
    "\n",
    "buckets_0_600 = optimize_buckets(0, 600, 5, df)\n",
    "buckets_600_850 = optimize_buckets(600, 850, 5, df)\n",
    "\n",
    "print(\"Buckets for 0-600 (Format: (Start, End, Log Likelihood)):\")\n",
    "for bucket in buckets_0_600:\n",
    "    print(bucket)\n",
    "\n",
    "print(\"\\nBuckets for 600-850 (Format: (Start, End, Log Likelihood)):\")\n",
    "for bucket in buckets_600_850:\n",
    "    print(bucket)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
